#!/usr/bin/env python3
"""
Sistema de Benchmark para An√°lise de Sentimento
Compara performance entre Ollama LLM e m√©todos tradicionais
"""

import time
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import numpy as np

# Importar analisadores
try:
    from ..sentiment.enhanced_sentiment_analyzer import EnhancedSentimentAnalyzer
    ENHANCED_AVAILABLE = True
except ImportError:
    print("Analisador aprimorado n√£o dispon√≠vel")
    ENHANCED_AVAILABLE = False

try:
    from .test_ollama_simple import test_ollama_direct
    SIMPLE_OLLAMA_AVAILABLE = True
except ImportError:
    print("Teste simples Ollama n√£o dispon√≠vel")
    SIMPLE_OLLAMA_AVAILABLE = False

@dataclass
class BenchmarkResult:
    """Resultado de benchmark"""
    text: str
    expected_sentiment: str
    
    # Resultados dos modelos
    ollama_sentiment: str
    ollama_confidence: float
    ollama_time: float
    
    traditional_sentiment: str = "neutral"
    traditional_confidence: float = 0.0
    traditional_time: float = 0.0
    
    # M√©tricas
    ollama_correct: bool = False
    traditional_correct: bool = False
    
    # Metadados
    timestamp: str = ""

class SentimentBenchmark:
    """Sistema de benchmark para an√°lise de sentimento"""
    
    def __init__(self):
        """Inicializa o sistema de benchmark"""
        self.analyzer = None
        self.results = []
        
        if ENHANCED_AVAILABLE:
            try:
                self.analyzer = EnhancedSentimentAnalyzer()
                print("‚úÖ Analisador aprimorado inicializado")
            except Exception as e:
                print(f"‚ùå Erro ao inicializar analisador: {e}")
    
    def create_test_dataset(self) -> List[Tuple[str, str]]:
        """
        Cria dataset de teste com sentimentos conhecidos
        
        Returns:
            Lista de (texto, sentimento_esperado)
        """
        test_data = [
            # Sentimentos POSITIVOS
            ("Bitcoin is going to the moon! Best investment ever!", "positive"),
            ("HODL! Diamond hands! Bitcoin will reach $100k soon!", "positive"),
            ("Bitcoin just broke all-time high! Incredible gains!", "positive"),
            ("This Bitcoin rally is amazing! Buying more!", "positive"),
            ("Bitcoin adoption is growing fast! Bullish!", "positive"),
            ("Just made huge profits on Bitcoin! To the moon!", "positive"),
            ("Bitcoin is the future of money! Revolutionary!", "positive"),
            ("Institutional investors are buying Bitcoin! Bullish signal!", "positive"),
            ("Bitcoin network is stronger than ever! Optimistic!", "positive"),
            ("Bitcoin price surge is just the beginning!", "positive"),
            
            # Sentimentos NEGATIVOS
            ("Bitcoin is crashing! Worst investment ever! I lost everything!", "negative"),
            ("This Bitcoin dump is terrible, selling everything now.", "negative"),
            ("Bitcoin is a scam! Don't invest in this bubble!", "negative"),
            ("Bitcoin crash wiped out my savings! Disaster!", "negative"),
            ("Bitcoin is dead! No future for this coin!", "negative"),
            ("Massive Bitcoin selloff! Market is collapsing!", "negative"),
            ("Bitcoin regulation will kill the market! Bearish!", "negative"),
            ("Bitcoin energy consumption is destroying the planet!", "negative"),
            ("Bitcoin volatility is too risky! Stay away!", "negative"),
            ("Bitcoin whale manipulation is ruining the market!", "negative"),
            
            # Sentimentos NEUTROS
            ("Bitcoin price is stable today, no major movements.", "neutral"),
            ("Bitcoin trading volume is normal for this time.", "neutral"),
            ("Bitcoin price analysis shows mixed signals.", "neutral"),
            ("Bitcoin market cap remains unchanged today.", "neutral"),
            ("Bitcoin technical indicators are inconclusive.", "neutral"),
            ("Bitcoin price consolidating in current range.", "neutral"),
            ("Bitcoin market showing sideways movement.", "neutral"),
            ("Bitcoin price action is range-bound today.", "neutral"),
            ("Bitcoin volatility decreased compared to yesterday.", "neutral"),
            ("Bitcoin market waiting for next catalyst.", "neutral"),
        ]
        
        return test_data
    
    def run_benchmark(self) -> List[BenchmarkResult]:
        """
        Executa benchmark completo
        
        Returns:
            Lista de BenchmarkResult
        """
        if not self.analyzer:
            print("‚ùå Analisador n√£o dispon√≠vel")
            return []
        
        test_data = self.create_test_dataset()
        results = []
        
        print(f"üöÄ Iniciando benchmark com {len(test_data)} textos...")
        print("="*60)
        
        for i, (text, expected) in enumerate(test_data, 1):
            print(f"\n[{i}/{len(test_data)}] Analisando: {text[:50]}...")
            
            try:
                # An√°lise com analisador aprimorado
                start_time = time.time()
                enhanced_result = self.analyzer.analyze_sentiment(text)
                total_time = time.time() - start_time
                
                # Verificar acur√°cia
                ollama_correct = enhanced_result.ollama_sentiment == expected
                final_correct = enhanced_result.final_sentiment == expected
                
                result = BenchmarkResult(
                    text=text,
                    expected_sentiment=expected,
                    ollama_sentiment=enhanced_result.ollama_sentiment,
                    ollama_confidence=enhanced_result.ollama_confidence,
                    ollama_time=enhanced_result.ollama_time,
                    traditional_sentiment=enhanced_result.final_sentiment,
                    traditional_confidence=enhanced_result.final_confidence,
                    traditional_time=total_time,
                    ollama_correct=ollama_correct,
                    traditional_correct=final_correct,
                    timestamp=datetime.now().isoformat()
                )
                
                results.append(result)
                
                # Feedback em tempo real
                ollama_status = "‚úÖ" if ollama_correct else "‚ùå"
                final_status = "‚úÖ" if final_correct else "‚ùå"
                
                print(f"  Esperado: {expected}")
                print(f"  Ollama: {enhanced_result.ollama_sentiment} {ollama_status} (conf: {enhanced_result.ollama_confidence:.2f})")
                print(f"  Final: {enhanced_result.final_sentiment} {final_status} (conf: {enhanced_result.final_confidence:.2f})")
                print(f"  Tempo: {enhanced_result.ollama_time:.2f}s")
                
            except Exception as e:
                print(f"  ‚ùå Erro: {e}")
                continue
        
        self.results = results
        return results
    
    def calculate_metrics(self, results: List[BenchmarkResult]) -> Dict:
        """Calcula m√©tricas de performance"""
        if not results:
            return {}
        
        # Acur√°cia
        ollama_accuracy = sum(r.ollama_correct for r in results) / len(results)
        traditional_accuracy = sum(r.traditional_correct for r in results) / len(results)
        
        # Tempo m√©dio
        avg_ollama_time = np.mean([r.ollama_time for r in results])
        avg_traditional_time = np.mean([r.traditional_time for r in results])
        
        # Confian√ßa m√©dia
        avg_ollama_confidence = np.mean([r.ollama_confidence for r in results])
        avg_traditional_confidence = np.mean([r.traditional_confidence for r in results])
        
        # Acur√°cia por sentimento
        by_sentiment = {}
        for sentiment in ['positive', 'negative', 'neutral']:
            sentiment_results = [r for r in results if r.expected_sentiment == sentiment]
            if sentiment_results:
                ollama_acc = sum(r.ollama_correct for r in sentiment_results) / len(sentiment_results)
                trad_acc = sum(r.traditional_correct for r in sentiment_results) / len(sentiment_results)
                by_sentiment[sentiment] = {
                    'ollama_accuracy': ollama_acc,
                    'traditional_accuracy': trad_acc,
                    'count': len(sentiment_results)
                }
        
        return {
            'overall': {
                'ollama_accuracy': ollama_accuracy,
                'traditional_accuracy': traditional_accuracy,
                'avg_ollama_time': avg_ollama_time,
                'avg_traditional_time': avg_traditional_time,
                'avg_ollama_confidence': avg_ollama_confidence,
                'avg_traditional_confidence': avg_traditional_confidence,
                'total_samples': len(results)
            },
            'by_sentiment': by_sentiment
        }
    
    def generate_report(self, results: List[BenchmarkResult], metrics: Dict) -> str:
        """Gera relat√≥rio detalhado"""
        report = []
        report.append("="*80)
        report.append("üìä RELAT√ìRIO DE BENCHMARK - AN√ÅLISE DE SENTIMENTO")
        report.append("="*80)
        report.append(f"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"Total de amostras: {metrics['overall']['total_samples']}")
        report.append("")
        
        # M√©tricas gerais
        report.append("üéØ M√âTRICAS GERAIS")
        report.append("-" * 40)
        report.append(f"Acur√°cia Ollama LLM:     {metrics['overall']['ollama_accuracy']:.1%}")
        report.append(f"Acur√°cia Sistema Final:  {metrics['overall']['traditional_accuracy']:.1%}")
        report.append(f"Tempo m√©dio Ollama:      {metrics['overall']['avg_ollama_time']:.2f}s")
        report.append(f"Tempo m√©dio Total:       {metrics['overall']['avg_traditional_time']:.2f}s")
        report.append(f"Confian√ßa m√©dia Ollama:  {metrics['overall']['avg_ollama_confidence']:.2f}")
        report.append(f"Confian√ßa m√©dia Final:   {metrics['overall']['avg_traditional_confidence']:.2f}")
        report.append("")
        
        # M√©tricas por sentimento
        report.append("üìà ACUR√ÅCIA POR SENTIMENTO")
        report.append("-" * 40)
        for sentiment, data in metrics['by_sentiment'].items():
            report.append(f"{sentiment.upper()}:")
            report.append(f"  Ollama:  {data['ollama_accuracy']:.1%} ({data['count']} amostras)")
            report.append(f"  Final:   {data['traditional_accuracy']:.1%}")
            report.append("")
        
        # An√°lise comparativa
        report.append("üîç AN√ÅLISE COMPARATIVA")
        report.append("-" * 40)
        
        ollama_acc = metrics['overall']['ollama_accuracy']
        final_acc = metrics['overall']['traditional_accuracy']
        
        if final_acc > ollama_acc:
            improvement = (final_acc - ollama_acc) * 100
            report.append(f"‚úÖ Sistema final {improvement:.1f}% mais preciso que Ollama sozinho")
        elif ollama_acc > final_acc:
            degradation = (ollama_acc - final_acc) * 100
            report.append(f"‚ö†Ô∏è  Ollama sozinho {degradation:.1f}% mais preciso que sistema final")
        else:
            report.append("üîÑ Performance similar entre Ollama e sistema final")
        
        # Recomenda√ß√µes
        report.append("")
        report.append("üí° RECOMENDA√á√ïES")
        report.append("-" * 40)
        
        if ollama_acc >= 0.8:
            report.append("‚úÖ Ollama LLM mostra excelente performance para an√°lise de sentimento")
        elif ollama_acc >= 0.6:
            report.append("‚ö†Ô∏è  Ollama LLM mostra performance moderada, considere fine-tuning")
        else:
            report.append("‚ùå Ollama LLM precisa de melhorias significativas")
        
        if metrics['overall']['avg_ollama_time'] > 10:
            report.append("‚è±Ô∏è  Tempo de resposta do Ollama pode ser otimizado")
        
        if metrics['overall']['avg_ollama_confidence'] < 0.7:
            report.append("üéØ Confian√ßa do modelo pode ser melhorada com prompts mais espec√≠ficos")
        
        return "\n".join(report)
    
    def save_results(self, results: List[BenchmarkResult], filename: str = "benchmark_results.json"):
        """Salva resultados em arquivo JSON"""
        data = [asdict(result) for result in results]
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        print(f"üíæ Resultados salvos em {filename}")
    
    def create_visualizations(self, results: List[BenchmarkResult], metrics: Dict):
        """Cria visualiza√ß√µes dos resultados"""
        if not results:
            return
        
        # Configurar estilo
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Benchmark An√°lise de Sentimento - Ollama vs Tradicional', fontsize=16, fontweight='bold')
        
        # 1. Acur√°cia por modelo
        models = ['Ollama LLM', 'Sistema Final']
        accuracies = [metrics['overall']['ollama_accuracy'], metrics['overall']['traditional_accuracy']]
        
        axes[0,0].bar(models, accuracies, color=['#FF6B6B', '#4ECDC4'])
        axes[0,0].set_title('Acur√°cia Geral')
        axes[0,0].set_ylabel('Acur√°cia')
        axes[0,0].set_ylim(0, 1)
        for i, v in enumerate(accuracies):
            axes[0,0].text(i, v + 0.02, f'{v:.1%}', ha='center', fontweight='bold')
        
        # 2. Tempo de processamento
        times = [metrics['overall']['avg_ollama_time'], metrics['overall']['avg_traditional_time']]
        axes[0,1].bar(models, times, color=['#FFE66D', '#FF6B6B'])
        axes[0,1].set_title('Tempo M√©dio de Processamento')
        axes[0,1].set_ylabel('Tempo (segundos)')
        for i, v in enumerate(times):
            axes[0,1].text(i, v + 0.1, f'{v:.2f}s', ha='center', fontweight='bold')
        
        # 3. Acur√°cia por sentimento
        sentiments = list(metrics['by_sentiment'].keys())
        ollama_by_sentiment = [metrics['by_sentiment'][s]['ollama_accuracy'] for s in sentiments]
        final_by_sentiment = [metrics['by_sentiment'][s]['traditional_accuracy'] for s in sentiments]
        
        x = np.arange(len(sentiments))
        width = 0.35
        
        axes[1,0].bar(x - width/2, ollama_by_sentiment, width, label='Ollama LLM', color='#FF6B6B')
        axes[1,0].bar(x + width/2, final_by_sentiment, width, label='Sistema Final', color='#4ECDC4')
        axes[1,0].set_title('Acur√°cia por Tipo de Sentimento')
        axes[1,0].set_ylabel('Acur√°cia')
        axes[1,0].set_xticks(x)
        axes[1,0].set_xticklabels([s.capitalize() for s in sentiments])
        axes[1,0].legend()
        axes[1,0].set_ylim(0, 1)
        
        # 4. Distribui√ß√£o de confian√ßa
        ollama_confidences = [r.ollama_confidence for r in results]
        final_confidences = [r.traditional_confidence for r in results]
        
        axes[1,1].hist(ollama_confidences, alpha=0.7, label='Ollama LLM', color='#FF6B6B', bins=10)
        axes[1,1].hist(final_confidences, alpha=0.7, label='Sistema Final', color='#4ECDC4', bins=10)
        axes[1,1].set_title('Distribui√ß√£o de Confian√ßa')
        axes[1,1].set_xlabel('Confian√ßa')
        axes[1,1].set_ylabel('Frequ√™ncia')
        axes[1,1].legend()
        
        plt.tight_layout()
        plt.savefig('benchmark_results.png', dpi=300, bbox_inches='tight')
        print("üìä Visualiza√ß√µes salvas em benchmark_results.png")
        plt.show()

def run_full_benchmark():
    """Executa benchmark completo"""
    print("üöÄ Iniciando Benchmark Completo de An√°lise de Sentimento")
    print("="*60)
    
    benchmark = SentimentBenchmark()
    
    # Executar benchmark
    results = benchmark.run_benchmark()
    
    if not results:
        print("‚ùå Nenhum resultado obtido")
        return
    
    # Calcular m√©tricas
    metrics = benchmark.calculate_metrics(results)
    
    # Gerar relat√≥rio
    report = benchmark.generate_report(results, metrics)
    print("\n" + report)
    
    # Salvar resultados
    benchmark.save_results(results)
    
    # Criar visualiza√ß√µes
    try:
        benchmark.create_visualizations(results, metrics)
    except Exception as e:
        print(f"‚ö†Ô∏è  Erro ao criar visualiza√ß√µes: {e}")
    
    # Salvar relat√≥rio
    with open('benchmark_report.txt', 'w', encoding='utf-8') as f:
        f.write(report)
    print("üìÑ Relat√≥rio salvo em benchmark_report.txt")
    
    return results, metrics

if __name__ == "__main__":
    run_full_benchmark()

